{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch to create a self driving TORCS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "EPOCHS = 100\n",
    "INITIAL_BATCH_SIZE = 1024\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001 * math.sqrt(BATCH_SIZE / INITIAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files in new_data folder\n",
    "files = os.listdir('new_data')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all csv files from new_data folder\n",
    "df = pd.read_csv(f\"new_data/{files[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(f'new_data/{file}') for file in files[1:]]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PyTorch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_gear_value(x):\n",
    "#     return (x + 1) / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Gear\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Gear\"] = df[\"Gear\"].apply(convert_gear_value)\n",
    "# df[\"Gear\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only the columns we need\n",
    "x_columns = [\n",
    "    \"Angle\",\n",
    "    \" DistanceCovered\",  # Distance raced\n",
    "    # \" Gear\",\n",
    "    \" Opponent_1\", \"Opponent_2\", \"Opponent_3\", \"Opponent_4\", \"Opponent_5\", \"Opponent_6\", \"Opponent_7\", \"Opponent_8\", \"Opponent_9\", \"Opponent_10\", \"Opponent_11\", \"Opponent_12\", \"Opponent_13\", \"Opponent_14\", \"Opponent_15\", \"Opponent_16\", \"Opponent_17\", \"Opponent_18\", \"Opponent_19\", \"Opponent_20\", \"Opponent_21\", \"Opponent_22\", \"Opponent_23\", \"Opponent_24\", \"Opponent_25\", \"Opponent_26\", \"Opponent_27\", \"Opponent_28\", \"Opponent_29\", \"Opponent_30\", \"Opponent_31\", \"Opponent_32\", \"Opponent_33\", \"Opponent_34\", \"Opponent_35\", \"Opponent_36\",\n",
    "    \" RPM\",\n",
    "    \" SpeedX\",\n",
    "    \" SpeedY\",\n",
    "    \" SpeedZ\",\n",
    "    \" Track_1\", \"Track_2\", \"Track_3\", \"Track_4\", \"Track_5\", \"Track_6\", \"Track_7\", \"Track_8\", \"Track_9\", \"Track_10\", \"Track_11\", \"Track_12\", \"Track_13\", \"Track_14\", \"Track_15\", \"Track_16\", \"Track_17\", \"Track_18\", \"Track_19\",\n",
    "    \"TrackPosition\",\n",
    "    \" WheelSpinVelocity_1\",\n",
    "    \"WheelSpinVelocity_2\",\n",
    "    \"WheelSpinVelocity_3\",\n",
    "    \"WheelSpinVelocity_4\",\n",
    "    \"Z\",\n",
    "]\n",
    "y_columns = [\" Acceleration\", \"Braking\", \"Steering\", \"Clutch\"]\n",
    "# y_columns = [\" Acceleration\", \"Braking\", \"Steering\", \"Gear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for features and target\n",
    "features = df[x_columns]\n",
    "targets = df[y_columns]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2,)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.fc7 = nn.Linear(32, 16)\n",
    "        self.fc8 = nn.Linear(16, 8)\n",
    "        self.fc9 = nn.Linear(8, output_size)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x))\n",
    "        x = self.fc9(x)\n",
    "        return x\n",
    "    \n",
    "model = Model(X_train.shape[1], y_train.shape[1]).to(device)\n",
    "print(X_train.shape[1], y_train.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss = []\n",
    "test_losses = []\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "\n",
    "def evaluate_in_batches(model, data_loader, device):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    return np.concatenate(all_preds), np.concatenate(all_targets)\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Evaluation\n",
    "    train_preds, train_targets = evaluate_in_batches(model, train_loader, device)\n",
    "    test_preds, test_targets = evaluate_in_batches(model, test_loader, device)\n",
    "\n",
    "    train_r2_score = r2_score(train_targets, train_preds)\n",
    "    test_r2_score = r2_score(test_targets, test_preds)\n",
    "\n",
    "    train_r2.append(train_r2_score)\n",
    "    test_r2.append(test_r2_score)\n",
    "\n",
    "    test_preds_tensor = torch.tensor(test_preds, device=device)\n",
    "    test_targets_tensor = torch.tensor(test_targets, device=device)\n",
    "    test_loss = criterion(test_preds_tensor, test_targets_tensor).item()\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {epoch_loss}, Test Loss: {test_loss}, Train R2: {train_r2_score}, Test R2: {test_r2_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "ax1.plot(train_loss, label='Train')\n",
    "ax1.plot(test_losses, label='Test')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_r2, label='Train')\n",
    "ax2.plot(test_r2, label='Test')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "fig.suptitle(f'Batch Size: {BATCH_SIZE}, Learning Rate: {LEARNING_RATE}')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'graphs/loss_accuracy_{BATCH_SIZE}_{datetime.now().strftime(\"%d_%m\")}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Save model with timestamp\n",
    "model_path = f'torcs-agent/models/model_{timestamp}.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save scaler with timestamp\n",
    "scaler_path = f'torcs-agent/models/scaler_{timestamp}.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
